# Experimentation Process

## In the course of developing this project, I conducted a series of experiments to optimize the neural network model. Initially, I explored training without a 2D pooling layer, which led to faster epochs but a slight accuracy drop to around 93%. Among various pool sizes, (2, 2) yielded the best balance between speed and accuracy. Lowering the dropout rate to 0.1 improved accuracy. Interestingly, additional layers extended training times without significant output enhancement. Notably, introducing a MaxPooling2D layer (2, 2) and an extra 256-unit layer with 0.1 dropout significantly boosted accuracy. The final configuration involves these modifications along with a softmax activation in the output layer, achieving a well-rounded model that excels in both training efficiency and prediction accuracy.
